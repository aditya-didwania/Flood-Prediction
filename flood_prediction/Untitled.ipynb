{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('updated_test.csv')\n",
    "dataset = dataset[ dataset['YEAR']>1980 ]\n",
    "dataset = dataset.dropna()\n",
    "X = dataset.iloc[:,[0,3,4,6,7]].values\n",
    "y = dataset.iloc[:,5].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data pre-processing\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_X = LabelEncoder()\n",
    "X[:, 0] = labelencoder_X.fit_transform(X[:, 0])\n",
    "X[:, 1] = labelencoder_X.fit_transform(X[:, 1])\n",
    "X[:, 3] = labelencoder_X.fit_transform(X[:, 3])\n",
    "X[:, 4] = labelencoder_X.fit_transform(X[:, 4])\n",
    "onehotencoder = OneHotEncoder(categorical_features = [0,1,3])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "X = X[:,1:]\n",
    "labelencoder_y = LabelEncoder()\n",
    "y = labelencoder_y.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECISION-TREE ACCURACY: 93.67541766109785\n"
     ]
    }
   ],
   "source": [
    "#decidion-Tree\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2,random_state = 0)\n",
    "onehotencoder = OneHotEncoder(categorical_features = [0])\n",
    "y_train = np.reshape(y_train,(-1,1))\n",
    "y_train = onehotencoder.fit_transform(y_train).toarray()\n",
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#normalization\n",
    "#val-mean/n\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n",
    "DTclassifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "DTclassifier.fit(X_train, y_train)\n",
    "\n",
    "#-p+logp+-p-logp-\n",
    "# Predicting the Test set results\n",
    "y_pred_dt = DTclassifier.predict(X_test)\n",
    "res_dt = np.argmax(y_pred_dt, axis=1)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "\n",
    "score = accuracy_score(y_test, res_dt)\n",
    "print(\"DECISION-TREE ACCURACY:\",score*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGISTIC REGRESSION ACCURACY: 93.7947494033413\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2,random_state = 0)\n",
    "onehotencoder = OneHotEncoder(categorical_features = [0])\n",
    "lr=LogisticRegression()\n",
    "lr.fit(X_train,y_train)\n",
    "y_pred_lr=lr.predict(X_test)\n",
    "score = lr.score(X_test, y_test)\n",
    "print(\"LOGISTIC REGRESSION ACCURACY:\", score*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM ACCURACY: 94.1527446300716\n"
     ]
    }
   ],
   "source": [
    "#SVM Model\n",
    "from sklearn import svm\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2,random_state = 0)\n",
    "onehotencoder = OneHotEncoder(categorical_features = [0])\n",
    "#y_train = np.reshape(y_train,(-1,1))\n",
    "#y_train = onehotencoder.fit_transform(y_train).toarray()\n",
    "\n",
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#normalization\n",
    "#val-mean/n\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n",
    "\n",
    "#Create a svm Classifier\n",
    "clf = svm.SVC(kernel='linear') # Linear Kernel\n",
    "\n",
    "#Train the model using the training sets\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred_svm = clf.predict(X_test)\n",
    "from sklearn import metrics\n",
    "print(\"SVM ACCURACY:\",metrics.accuracy_score(y_test, y_pred_svm)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANN\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2,random_state = 0)\n",
    "onehotencoder = OneHotEncoder(categorical_features = [0])\n",
    "y_train = np.reshape(y_train,(-1,1))\n",
    "y_train = onehotencoder.fit_transform(y_train).toarray()\n",
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#normalization\n",
    "#val-mean/n\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Initialising the ANN\n",
    "classifier = Sequential()\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(units=32, input_dim=47, activation=\"relu\", kernel_initializer=\"uniform\"))\n",
    "#uniform,zero,normal\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(units=16, activation=\"relu\", kernel_initializer=\"uniform\"))\n",
    "\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(units=6, activation=\"softmax\", kernel_initializer=\"uniform\"))\n",
    "\n",
    "# Compiling the ANN\n",
    "#classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "classifier.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.4673 - acc: 0.9406     \n",
      "Epoch 2/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1698 - acc: 0.9424     \n",
      "Epoch 3/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1618 - acc: 0.9424     - ETA: 1s - lo\n",
      "Epoch 4/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1541 - acc: 0.9424     \n",
      "Epoch 5/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1506 - acc: 0.9424     \n",
      "Epoch 6/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1478 - acc: 0.9424     \n",
      "Epoch 7/100\n",
      "3349/3349 [==============================] - ETA: 0s - loss: 0.1440 - acc: 0.9429- ETA: 1s - - 1s - loss: 0.1449 - acc: 0.9424     \n",
      "Epoch 8/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1438 - acc: 0.9427     \n",
      "Epoch 9/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1405 - acc: 0.9454     \n",
      "Epoch 10/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1391 - acc: 0.9460     \n",
      "Epoch 11/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1381 - acc: 0.9442     \n",
      "Epoch 12/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1349 - acc: 0.9466     \n",
      "Epoch 13/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1346 - acc: 0.9445     \n",
      "Epoch 14/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1338 - acc: 0.9460     \n",
      "Epoch 15/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1326 - acc: 0.9457     \n",
      "Epoch 16/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1307 - acc: 0.9457     \n",
      "Epoch 17/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1299 - acc: 0.9471     \n",
      "Epoch 18/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1282 - acc: 0.9463     \n",
      "Epoch 19/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1273 - acc: 0.9480     \n",
      "Epoch 20/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1264 - acc: 0.9466     \n",
      "Epoch 21/100\n",
      "3349/3349 [==============================] - ETA: 0s - loss: 0.1254 - acc: 0.946 - 1s - loss: 0.1248 - acc: 0.9463     \n",
      "Epoch 22/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1243 - acc: 0.9483     \n",
      "Epoch 23/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1223 - acc: 0.9474     \n",
      "Epoch 24/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1220 - acc: 0.9501     \n",
      "Epoch 25/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1212 - acc: 0.9498     \n",
      "Epoch 26/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1220 - acc: 0.9489     \n",
      "Epoch 27/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1201 - acc: 0.9486     \n",
      "Epoch 28/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1188 - acc: 0.9507     \n",
      "Epoch 29/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1171 - acc: 0.9510     \n",
      "Epoch 30/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1167 - acc: 0.9483     \n",
      "Epoch 31/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1156 - acc: 0.9516     \n",
      "Epoch 32/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1160 - acc: 0.9480     \n",
      "Epoch 33/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1140 - acc: 0.9495     \n",
      "Epoch 34/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1147 - acc: 0.9510     \n",
      "Epoch 35/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1130 - acc: 0.9510     \n",
      "Epoch 36/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1132 - acc: 0.9519     \n",
      "Epoch 37/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1123 - acc: 0.9543     \n",
      "Epoch 38/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1115 - acc: 0.9531     \n",
      "Epoch 39/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1114 - acc: 0.9525     - ETA: 0s - loss: 0.1152 - a\n",
      "Epoch 40/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1119 - acc: 0.9519     \n",
      "Epoch 41/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1094 - acc: 0.9552     \n",
      "Epoch 42/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1118 - acc: 0.9516     \n",
      "Epoch 43/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1096 - acc: 0.9534     \n",
      "Epoch 44/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1094 - acc: 0.9561     \n",
      "Epoch 45/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1086 - acc: 0.9537     \n",
      "Epoch 46/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1098 - acc: 0.9558     \n",
      "Epoch 47/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1093 - acc: 0.9522     - ETA: 0s - loss: 0.10\n",
      "Epoch 48/100\n",
      "3349/3349 [==============================] - 3s - loss: 0.1082 - acc: 0.9555     - ETA: 1s - los\n",
      "Epoch 49/100\n",
      "3349/3349 [==============================] - 2s - loss: 0.1088 - acc: 0.9555     \n",
      "Epoch 50/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1079 - acc: 0.9534     \n",
      "Epoch 51/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1074 - acc: 0.9552     \n",
      "Epoch 52/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1072 - acc: 0.9558     - ETA: 1s - \n",
      "Epoch 53/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1083 - acc: 0.9561     \n",
      "Epoch 54/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1059 - acc: 0.9570     \n",
      "Epoch 55/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1072 - acc: 0.9570     \n",
      "Epoch 56/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1064 - acc: 0.9564     \n",
      "Epoch 57/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1068 - acc: 0.9576     \n",
      "Epoch 58/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1075 - acc: 0.9555     \n",
      "Epoch 59/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1078 - acc: 0.9570     \n",
      "Epoch 60/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1049 - acc: 0.9573     \n",
      "Epoch 61/100\n",
      "3349/3349 [==============================] - ETA: 0s - loss: 0.1079 - acc: 0.955 - 1s - loss: 0.1077 - acc: 0.9558     \n",
      "Epoch 62/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1050 - acc: 0.9579     \n",
      "Epoch 63/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1052 - acc: 0.9552     \n",
      "Epoch 64/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1054 - acc: 0.9555     \n",
      "Epoch 65/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1041 - acc: 0.9567     \n",
      "Epoch 66/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1034 - acc: 0.9567     \n",
      "Epoch 67/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1041 - acc: 0.9579     \n",
      "Epoch 68/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1039 - acc: 0.9600     \n",
      "Epoch 69/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1038 - acc: 0.9570     \n",
      "Epoch 70/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1013 - acc: 0.9591     \n",
      "Epoch 71/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1056 - acc: 0.9555     \n",
      "Epoch 72/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1029 - acc: 0.9579     \n",
      "Epoch 73/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1037 - acc: 0.9576     \n",
      "Epoch 74/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1025 - acc: 0.9582     \n",
      "Epoch 75/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1024 - acc: 0.9576     \n",
      "Epoch 76/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1053 - acc: 0.9570     - ETA: 1s - loss: \n",
      "Epoch 77/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1021 - acc: 0.9579     - ETA: 1\n",
      "Epoch 78/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1027 - acc: 0.9555     \n",
      "Epoch 79/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1029 - acc: 0.9570     \n",
      "Epoch 80/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1035 - acc: 0.9558     \n",
      "Epoch 81/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1014 - acc: 0.9582     \n",
      "Epoch 82/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1027 - acc: 0.9594     \n",
      "Epoch 83/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1040 - acc: 0.9555     \n",
      "Epoch 84/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3349/3349 [==============================] - 1s - loss: 0.1013 - acc: 0.9585     \n",
      "Epoch 85/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1005 - acc: 0.9594     \n",
      "Epoch 86/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1013 - acc: 0.9591     \n",
      "Epoch 87/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1010 - acc: 0.9558     \n",
      "Epoch 88/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1010 - acc: 0.9582     \n",
      "Epoch 89/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1008 - acc: 0.9582     \n",
      "Epoch 90/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1003 - acc: 0.9570     \n",
      "Epoch 91/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1007 - acc: 0.9582     \n",
      "Epoch 92/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1008 - acc: 0.9576     \n",
      "Epoch 93/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.0996 - acc: 0.9567     \n",
      "Epoch 94/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1008 - acc: 0.9579     \n",
      "Epoch 95/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.0996 - acc: 0.9576     \n",
      "Epoch 96/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1006 - acc: 0.9561     - ETA: 0s - loss\n",
      "Epoch 97/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1006 - acc: 0.9582     \n",
      "Epoch 98/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.0984 - acc: 0.9606     \n",
      "Epoch 99/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.0992 - acc: 0.9600     \n",
      "Epoch 100/100\n",
      "3349/3349 [==============================] - 1s - loss: 0.1001 - acc: 0.9594     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2053963dcc0>"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_train, y_train, batch_size = 10, nb_epoch = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANN Accuracy: 94.63007159904535\n"
     ]
    }
   ],
   "source": [
    "y_pred_ann = classifier.predict(X_test)\n",
    "res_ann = np.argmax(y_pred_ann, axis=1)\n",
    "from sklearn.metrics import accuracy_score\n",
    "a = accuracy_score(res_ann,y_test)\n",
    "print('ANN Accuracy:', a*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
