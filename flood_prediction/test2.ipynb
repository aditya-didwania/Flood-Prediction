{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'(slice(4000, 4005, None), slice(None, None, None))' is an invalid key",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-d777ccc32cb0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0maccuracies\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4000\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m4005\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\aksha\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2993\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2994\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2995\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2996\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2997\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\aksha\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2895\u001b[0m                 )\n\u001b[0;32m   2896\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2897\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2898\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2899\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: '(slice(4000, 4005, None), slice(None, None, None))' is an invalid key"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "dataset = pd.read_csv('updated_test.csv')\n",
    "dataset = dataset[ dataset['YEAR']>1980 ]\n",
    "dataset = dataset.dropna()\n",
    "X = dataset.iloc[:,[0,3,4,6,7]].values\n",
    "y = dataset.iloc[:,5].values\n",
    "global accuracies\n",
    "accuracies=[]\n",
    "df=pd.DataFrame(dataset)\n",
    "print(df[4000:4005,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4187, 5)\n",
      "(4187, 48)\n"
     ]
    }
   ],
   "source": [
    "#data pre-processing\n",
    "print(X.shape)\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_X = LabelEncoder()\n",
    "X[:, 0] = labelencoder_X.fit_transform(X[:, 0])\n",
    "X[:, 1] = labelencoder_X.fit_transform(X[:, 1])\n",
    "X[:, 3] = labelencoder_X.fit_transform(X[:, 3])\n",
    "X[:, 4] = labelencoder_X.fit_transform(X[:, 4])\n",
    "onehotencoder = OneHotEncoder(categorical_features = [0,1,3])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "print(X.shape)\n",
    "X = X[:,1:]\n",
    "labelencoder_y = LabelEncoder()\n",
    "y = labelencoder_y.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3349, 47)\n",
      "(838, 47)\n",
      "Epoch 1/1\n",
      "3349/3349 [==============================] - 5s - loss: 0.5240 - acc: 0.9364     \n",
      ">Saved xyz_models/model_1.h5\n",
      "Epoch 1/1\n",
      "3349/3349 [==============================] - 5s - loss: 0.5109 - acc: 0.9379     \n",
      ">Saved xyz_models/model_2.h5\n",
      "Epoch 1/1\n",
      "3349/3349 [==============================] - 6s - loss: 0.4751 - acc: 0.9391     - ETA: 4s - \n",
      ">Saved xyz_models/model_3.h5\n",
      "Epoch 1/1\n",
      "3349/3349 [==============================] - 4s - loss: 0.5325 - acc: 0.9376     \n",
      ">Saved xyz_models/model_4.h5\n",
      "Epoch 1/1\n",
      "3349/3349 [==============================] - 7s - loss: 0.4853 - acc: 0.9403     \n",
      ">Saved xyz_models/model_5.h5\n"
     ]
    }
   ],
   "source": [
    "# example of saving sub-models for later use in a stacking ensemble\n",
    "from sklearn.datasets import make_blobs\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from matplotlib import pyplot\n",
    "from os import makedirs\n",
    "\n",
    "# fit model on dataset\n",
    "def fit_model(trainX, trainy):\n",
    "    # define model\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(units=32, input_dim=47, activation=\"relu\", kernel_initializer=\"uniform\"))\n",
    "    classifier.add(Dense(units=16, activation=\"relu\", kernel_initializer=\"uniform\"))\n",
    "    classifier.add(Dense(units=6, activation=\"softmax\", kernel_initializer=\"uniform\"))\n",
    "    classifier.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    classifier.fit(X_train, y_train, batch_size = 10, nb_epoch = 1)\n",
    "    return  classifier\n",
    "\n",
    "# generate 2d classification dataset\n",
    "\n",
    "# create directory for models\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2,random_state = 0)\n",
    "onehotencoder = OneHotEncoder(categorical_features = [0])\n",
    "y_train = np.reshape(y_train,(-1,1))\n",
    "y_train = onehotencoder.fit_transform(y_train).toarray()\n",
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#normalization\n",
    "#val-mean/n\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "makedirs('xyz_models')\n",
    "# fit and save models\n",
    "n_members = 5\n",
    "for i in range(n_members):\n",
    "\t# fit model\n",
    "\tclassifier = fit_model(X_train, y_train)\n",
    "\t# save model\n",
    "\tfilename = 'xyz_models/model_' + str(i + 1) + '.h5'\n",
    "\tclassifier.save(filename)\n",
    "\tprint('>Saved %s' % filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">loaded xyz_models/model_1.h5\n",
      ">loaded xyz_models/model_2.h5\n",
      ">loaded xyz_models/model_3.h5\n",
      ">loaded xyz_models/model_4.h5\n",
      ">loaded xyz_models/model_5.h5\n",
      "Loaded 5 models\n",
      "(838,)\n",
      "(838, 47)\n",
      "(838, 6)\n",
      "Model Accuracy: 0.937\n",
      "(838,)\n",
      "(838, 47)\n",
      "(838, 6)\n",
      "Model Accuracy: 0.937\n",
      "(838,)\n",
      "(838, 47)\n",
      "(838, 6)\n",
      "Model Accuracy: 0.937\n",
      "(838,)\n",
      "(838, 47)\n",
      "(838, 6)\n",
      "Model Accuracy: 0.937\n",
      "(838,)\n",
      "(838, 47)\n",
      "(838, 6)\n",
      "Model Accuracy: 0.937\n",
      "After train (838, 47)\n",
      "(838, 47)\n",
      "Stacked Test Accuracy: 0.937\n"
     ]
    }
   ],
   "source": [
    "# stacked generalization with linear meta model on blobs dataset\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.models import load_model\n",
    "from keras.utils import to_categorical\n",
    "from numpy import dstack\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dataset = pd.read_csv('updated_test.csv')\n",
    "dataset = dataset[ dataset['YEAR']>1980 ]\n",
    "dataset = dataset.dropna()\n",
    "X = dataset.iloc[:,[0,3,4,6,7]].values\n",
    "y = dataset.iloc[:,5].values\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_X = LabelEncoder()\n",
    "X[:, 0] = labelencoder_X.fit_transform(X[:, 0])\n",
    "X[:, 1] = labelencoder_X.fit_transform(X[:, 1])\n",
    "X[:, 3] = labelencoder_X.fit_transform(X[:, 3])\n",
    "X[:, 4] = labelencoder_X.fit_transform(X[:, 4])\n",
    "onehotencoder = OneHotEncoder(categorical_features = [0,1,3])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "X = X[:,1:]\n",
    "labelencoder_y = LabelEncoder()\n",
    "y = labelencoder_y.fit_transform(y)\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2,random_state = 0)\n",
    "onehotencoder = OneHotEncoder(categorical_features = [0])\n",
    "y_train = np.reshape(y_train,(-1,1))\n",
    "y_train = onehotencoder.fit_transform(y_train).toarray()\n",
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#normalization\n",
    "#val-mean/n\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n",
    "\n",
    "# load models from file\n",
    "def load_all_models(n_models):\n",
    "\tall_models = list()\n",
    "\tfor i in range(n_models):\n",
    "\t\t# define filename for this ensemble\n",
    "\t\tfilename = 'xyz_models/model_' + str(i + 1) + '.h5'\n",
    "\t\t# load model from file\n",
    "\t\tmodel = load_model(filename)\n",
    "\t\t# add to list of members\n",
    "\t\tall_models.append(model)\n",
    "\t\tprint('>loaded %s' % filename)\n",
    "\treturn all_models\n",
    "\n",
    "# create stacked model input dataset as outputs from the ensemble\n",
    "def stacked_dataset(members, inputX):\n",
    "\tstackX = None\n",
    "\tfor model in members:\n",
    "\t\t# make prediction\n",
    "\t\tyhat = model.predict(inputX, verbose=0)\n",
    "\t\t# stack predictions into [rows, members, probabilities]\n",
    "\t\tif stackX is None:\n",
    "\t\t\tstackX = yhat\n",
    "\t\telse:\n",
    "\t\t\tstackX = dstack((stackX, yhat))\n",
    "\t# flatten predictions to [rows, members x probabilities]\n",
    "\tstackX = stackX.reshape((stackX.shape[0], stackX.shape[1]*stackX.shape[2]))\n",
    "\treturn stackX\n",
    "\n",
    "# fit a model based on the outputs from the ensemble members\n",
    "def fit_stacked_model(members, inputX, inputy):\n",
    "\t# create dataset using ensemble\n",
    "\tstackedX = stacked_dataset(members, inputX)\n",
    "\t# fit standalone model\n",
    "\tmodel = svm.SVC(kernel='linear')\n",
    "\tmodel.fit(stackedX, inputy)\n",
    "\treturn model\n",
    "\n",
    "# make a prediction with the stacked model\n",
    "def stacked_prediction(members, model, inputX):\n",
    "    # create dataset using ensemble\n",
    "    print(inputX.shape)\n",
    "    stackedX = stacked_dataset(members, inputX)\n",
    "    # make a prediction\n",
    "    yhat = model.predict(stackedX)\n",
    "    return yhat\n",
    "\n",
    "# load all models\n",
    "n_members = 5\n",
    "members = load_all_models(n_members)\n",
    "print('Loaded %d models' % len(members))\n",
    "# evaluate standalone models on test dataset\n",
    "for model in members:\n",
    "    testy_enc = to_categorical(y_test)\n",
    "    _, acc = model.evaluate(X_test, testy_enc, verbose=0)\n",
    "    print('Model Accuracy: %.3f' % acc)\n",
    "# fit stacked model using the ensemble\n",
    "model = fit_stacked_model(members, X_test, y_test)\n",
    "# evaluate model on test set\n",
    "yhat = stacked_prediction(members, model, X_test)\n",
    "acc = accuracy_score(y_test, yhat)\n",
    "print('Stacked Test Accuracy: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(838,)\n",
      "(838, 6)\n",
      "(838, 47)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'inputX' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-131-87110e791144>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtesty_enc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'inputX' is not defined"
     ]
    }
   ],
   "source": [
    "print(y_test.shape)\n",
    "print(testy_enc.shape)\n",
    "print(X_test.shape)\n",
    "testy_enc = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM ACCURACY: 0.942\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2,random_state = 0)\n",
    "onehotencoder = OneHotEncoder(categorical_features = [0])\n",
    "#y_train = np.reshape(y_train,(-1,1))\n",
    "#y_train = onehotencoder.fit_transform(y_train).toarray()\n",
    "\n",
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#normalization\n",
    "#val-mean/n\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n",
    "\n",
    "#Create a svm Classifier\n",
    "clf = svm.SVC(kernel='linear') # Linear Kernel\n",
    "\n",
    "#Train the model using the training sets\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred_svm = clf.predict(X_test)\n",
    "from sklearn import metrics\n",
    "global svm_score\n",
    "svm_score=metrics.accuracy_score(y_test, y_pred_svm)\n",
    "print(\"SVM ACCURACY: %.3f\" % svm_score)\n",
    "accuracies.append(svm_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    SUBDIVISION  YEAR  ANNUAL PRECIPITATION  QUARTER  \\\n",
      "0     ANDAMAN & NICOBAR ISLANDS  1980                3625.3  Jan-Feb   \n",
      "1     ANDAMAN & NICOBAR ISLANDS  1980                3625.3  Mar-May   \n",
      "2     ANDAMAN & NICOBAR ISLANDS  1980                3625.3  Jun-Sep   \n",
      "3     ANDAMAN & NICOBAR ISLANDS  1980                3625.3  Oct-Dec   \n",
      "4     ANDAMAN & NICOBAR ISLANDS  1981                3074.8  Jan-Feb   \n",
      "...                         ...   ...                   ...      ...   \n",
      "4311         WEST UTTAR PRADESH  2014                 486.9  Oct-Dec   \n",
      "4312         WEST UTTAR PRADESH  2015                 582.7  Jan-Feb   \n",
      "4313         WEST UTTAR PRADESH  2015                 582.7  Mar-May   \n",
      "4314         WEST UTTAR PRADESH  2015                 582.7  Jun-Sep   \n",
      "4315         WEST UTTAR PRADESH  2015                 582.7  Oct-Dec   \n",
      "\n",
      "      PRECIPITATION  SEVERITY1 TERRAIN   CLIMATE  SEVERITY  \n",
      "0               6.3          0  Island  Tropical         0  \n",
      "1             776.5          0  Island  Tropical         0  \n",
      "2            1946.3          4  Island  Tropical         4  \n",
      "3             896.3          0  Island  Tropical         0  \n",
      "4              38.9          0  Island  Tropical         0  \n",
      "...             ...        ...     ...       ...       ...  \n",
      "4311           30.8          0   Hilly  Extremes         0  \n",
      "4312           38.8          0   Hilly  Extremes         0  \n",
      "4313           95.9          0   Hilly  Extremes         0  \n",
      "4314          436.1          0   Hilly  Extremes         0  \n",
      "4315           11.9          0   Hilly  Extremes         0  \n",
      "\n",
      "[4316 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.models import load_model\n",
    "from keras.utils import to_categorical\n",
    "from numpy import dstack\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import tensorflow as tf\n",
    "dataset = pd.read_csv('updated_test.csv')\n",
    "print(dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4187, 5)\n",
      "['WEST RAJASTHAN' 'Mar-May' 10.8 'Desert' 'Hot and dry']\n",
      "[28 2 10.8 2 6]\n",
      "(4187, 5)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'OneHotEncoder' object has no attribute 'feature_indices_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-154-71d5b96c8172>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0monehotencoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcategorical_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0monehotencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nlength:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python35\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   2073\u001b[0m         \"\"\"\n\u001b[0;32m   2074\u001b[0m         return _transform_selected(X, self._transform,\n\u001b[1;32m-> 2075\u001b[1;33m                                    self.categorical_features, copy=True)\n\u001b[0m\u001b[0;32m   2076\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2077\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python35\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36m_transform_selected\u001b[1;34m(X, transform, selected, copy)\u001b[0m\n\u001b[0;32m   1829\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1830\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1831\u001b[1;33m         \u001b[0mX_sel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mind\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1832\u001b[0m         \u001b[0mX_not_sel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mind\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnot_sel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1833\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python35\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36m_transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   2026\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2027\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2028\u001b[1;33m         \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_indices_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2029\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2030\u001b[0m             raise ValueError(\"X has different shape than during fitting.\"\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'OneHotEncoder' object has no attribute 'feature_indices_'"
     ]
    }
   ],
   "source": [
    "dataset = dataset[ dataset['YEAR']>1980 ]\n",
    "dataset = dataset.dropna()\n",
    "X = dataset.iloc[:,[0,3,4,6,7]].values\n",
    "y = dataset.iloc[:,5].values\n",
    "print(X.shape)\n",
    "print(X[4000][:])\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_X = LabelEncoder()\n",
    "X[:, 0] = labelencoder_X.fit_transform(X[:, 0])\n",
    "X[:, 1] = labelencoder_X.fit_transform(X[:, 1])\n",
    "X[:, 3] = labelencoder_X.fit_transform(X[:, 3])\n",
    "X[:, 4] = labelencoder_X.fit_transform(X[:, 4])\n",
    "print(X[4000][:])\n",
    "print(X.shape)\n",
    "onehotencoder = OneHotEncoder(categorical_features = [0,1,3])\n",
    "X = onehotencoder.transform(X).toarray()\n",
    "print(\"\\nlength:\",len(X[4000][:]))\n",
    "print(X[4000][:])\n",
    "\n",
    "X = X[:,1:]\n",
    "print(\"\\nlength:\",len(X[4000][:]))\n",
    "print(X[4000][:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y\n",
      "length: 4187\n",
      "[0 0 0 ... 0 0 0]\n",
      "\n",
      "X_train\n",
      "(3349, 5)\n",
      "\n",
      "X_test\n",
      "(838, 5)\n",
      "\n",
      "y_train\n",
      "(3349,)\n",
      "\n",
      "y_test\n",
      "(838,)\n"
     ]
    }
   ],
   "source": [
    "labelencoder_y = LabelEncoder()\n",
    "y = labelencoder_y.fit_transform(y)\n",
    "print(\"y\")\n",
    "print(\"length:\",len(y))\n",
    "print(y)\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2,random_state = 0)\n",
    "onehotencoder = OneHotEncoder(categorical_features = [0])\n",
    "print('\\nX_train')\n",
    "print(X_train.shape)\n",
    "\n",
    "print('\\nX_test')\n",
    "print(X_test.shape)\n",
    "\n",
    "print('\\ny_train')\n",
    "print(y_train.shape)\n",
    "\n",
    "print('\\ny_test')\n",
    "print(y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "y_train\n",
      "(3349, 1)\n",
      "[0]\n",
      "\n",
      "\n",
      "(3349, 6)\n",
      "[1. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "y_train = np.reshape(y_train,(-1,1))\n",
    "print('\\ny_train')\n",
    "print(y_train.shape)\n",
    "print(y_train[0])\n",
    "y_train = onehotencoder.fit_transform(y_train).toarray()\n",
    "print(\"\\n\")\n",
    "print(y_train.shape)\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train\n",
      "[  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    1.\n",
      "   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "   0.    0.    0.    0.    0.    0.    0.    1.    0.    0.    0.    0.\n",
      "   0.    0.    0.    0.    0.    0.    0.    1.    0.  114.9   3. ]\n",
      "\n",
      "Normalization\n",
      "\n",
      "X_train\n",
      "[-0.18686801 -0.18857274 -0.17270726 -0.1790232  -0.19443768 -0.18167091\n",
      " -0.18254602 -0.19277761 -0.18254602 -0.18686801 -0.19026428  5.25584737\n",
      " -0.18601059 -0.18167091 -0.18428537 -0.18601059 -0.18772205 -0.18857274\n",
      " -0.18514972 -0.18857274 -0.18601059 -0.18428537 -0.18942014 -0.18772205\n",
      " -0.18514972 -0.18601059 -0.18254602 -0.18686801 -0.18167091 -0.5671207\n",
      " -0.5786145   1.7187074  -0.5818326  -0.50335699 -0.18686801 -0.27345082\n",
      " -0.19026428 -0.18254602 -0.18428537 -0.65293206 -0.18772205 -0.18514972\n",
      " -0.18341749  2.2608215  -0.18254602 -0.44929246 -1.09172701]\n",
      "\n",
      "X_test\n",
      "[-0.18686801 -0.18857274 -0.17270726 -0.1790232  -0.19443768 -0.18167091\n",
      " -0.18254602 -0.19277761 -0.18254602 -0.18686801 -0.19026428 -0.19026428\n",
      " -0.18601059 -0.18167091 -0.18428537  5.37603811 -0.18772205 -0.18857274\n",
      " -0.18514972 -0.18857274 -0.18601059 -0.18428537 -0.18942014 -0.18772205\n",
      " -0.18514972 -0.18601059 -0.18254602 -0.18686801 -0.18167091 -0.5671207\n",
      " -0.5786145  -0.5818326   1.7187074   1.98666158 -0.18686801 -0.27345082\n",
      " -0.19026428 -0.18254602 -0.18428537 -0.65293206 -0.18772205 -0.18514972\n",
      " -0.18341749 -0.4423171  -0.18254602  0.24889376 -0.32703289]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#normalization\n",
    "#val-mean/n\n",
    "print(\"X_train\")\n",
    "print(X_train[0][:])\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "print(\"\\nNormalization\")\n",
    "print(\"\\nX_train\")\n",
    "print(X_train[0][:])\n",
    "X_test = sc_X.transform(X_test)\n",
    "print(\"\\nX_test\")\n",
    "print(X_test[0][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load models from file\n",
    "def load_all_models(n_models):\n",
    "\tall_models = list()\n",
    "\tfor i in range(n_models):\n",
    "\t\t# define filename for this ensemble\n",
    "\t\tfilename = 'main_models/model_' + str(i + 1) + '.h5'\n",
    "\t\t# load model from file\n",
    "\t\tmodel = load_model(filename)\n",
    "\t\t# add to list of members\n",
    "\t\tall_models.append(model)\n",
    "\t\tprint('>loaded %s' % filename)\n",
    "\treturn all_models\n",
    "\n",
    "# create stacked model input dataset as outputs from the ensemble\n",
    "def stacked_dataset(members, inputX):\n",
    "\tstackX = None\n",
    "\tfor model in members:\n",
    "\t\t# make prediction\n",
    "\t\tyhat = model.predict(inputX, verbose=0)\n",
    "\t\t# stack predictions into [rows, members, probabilities]\n",
    "\t\tif stackX is None:\n",
    "\t\t\tstackX = yhat\n",
    "\t\telse:\n",
    "\t\t\tstackX = dstack((stackX, yhat))\n",
    "\t# flatten predictions to [rows, members x probabilities]\n",
    "\tstackX = stackX.reshape((stackX.shape[0], stackX.shape[1]*stackX.shape[2]))\n",
    "\treturn stackX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a model based on the outputs from the ensemble members\n",
    "def fit_stacked_model(members, inputX, inputy):\n",
    "    # create dataset using ensemble\n",
    "    print(\"fit_stacked_model\")\n",
    "    print(inputX.shape)\n",
    "    print(inputy.shape)\n",
    "    stackedX = stacked_dataset(members, inputX)\n",
    "    # fit standalone model\n",
    "    model = svm.SVC(kernel='linear')\n",
    "    model.fit(stackedX, inputy)\n",
    "    y_pred_svm = model.predict(i)\n",
    "    from sklearn import metrics\n",
    "    svm_score=metrics.accuracy_score(y_test, y_pred_svm)*100\n",
    "    print(\"SVM ACCURACY:\",svm_score)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacked_prediction(members, model, inputX):\n",
    "\t# create dataset using ensemble\n",
    "\tstackedX = stacked_dataset(members, inputX)\n",
    "\t# make a prediction\n",
    "\tyhat = model.predict(stackedX)\n",
    "\treturn yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">loaded main_models/model_1.h5\n",
      ">loaded main_models/model_2.h5\n",
      ">loaded main_models/model_3.h5\n",
      ">loaded main_models/model_4.h5\n",
      ">loaded main_models/model_5.h5\n",
      "Loaded 5 models\n",
      "(838,)\n",
      "(838, 6)\n",
      "Model Accuracy: 0.947\n",
      "(838,)\n",
      "(838, 6)\n",
      "Model Accuracy: 0.942\n",
      "(838,)\n",
      "(838, 6)\n",
      "Model Accuracy: 0.947\n",
      "(838,)\n",
      "(838, 6)\n",
      "Model Accuracy: 0.945\n",
      "(838,)\n",
      "(838, 6)\n",
      "Model Accuracy: 0.945\n",
      "fit_stacked_model\n",
      "(838, 47)\n",
      "(838,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X.shape[1] = 1 should be equal to 30, the number of features at training time",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-147-b58066af1f51>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtesty_enc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Model Accuracy: %.3f'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_stacked_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmembers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"done F_S_M\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0myhat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstacked_prediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmembers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-145-952b98256bc2>\u001b[0m in \u001b[0;36mfit_stacked_model\u001b[1;34m(members, inputX, inputy)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'linear'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstackedX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0my_pred_svm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0msvm_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred_svm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python35\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    546\u001b[0m             \u001b[0mClass\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m         \"\"\"\n\u001b[1;32m--> 548\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseSVC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    549\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python35\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    306\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m         \"\"\"\n\u001b[1;32m--> 308\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_for_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    309\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sparse_predict\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sparse\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dense_predict\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python35\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36m_validate_for_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    457\u001b[0m             raise ValueError(\"X.shape[1] = %d should be equal to %d, \"\n\u001b[0;32m    458\u001b[0m                              \u001b[1;34m\"the number of features at training time\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 459\u001b[1;33m                              (n_features, self.shape_fit_[1]))\n\u001b[0m\u001b[0;32m    460\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: X.shape[1] = 1 should be equal to 30, the number of features at training time"
     ]
    }
   ],
   "source": [
    "n_members = 5\n",
    "members = load_all_models(n_members)\n",
    "print('Loaded %d models' % len(members))\n",
    "# evaluate standalone models on test dataset\n",
    "\n",
    "for model in members:\n",
    "    testy_enc = to_categorical(y_test)\n",
    "    print(y_test.shape)\n",
    "    print(testy_enc.shape)\n",
    "    _, acc = model.evaluate(X_test, testy_enc, verbose=0)\n",
    "    print('Model Accuracy: %.3f' % acc)\n",
    "model = fit_stacked_model(members, X_test, y_test)\n",
    "print(\"done F_S_M\")\n",
    "yhat = stacked_prediction(members, model, X_test)\n",
    "acc = accuracy_score(y_test, yhat)\n",
    "print('Stacked Test Accuracy: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0.     0.     0.  ...    0.    38.9   16. ]\n",
      " [   0.     0.     0.  ...    0.   366.7   16. ]\n",
      " [   0.     0.     0.  ...    0.  1908.    16. ]\n",
      " ...\n",
      " [   0.     0.     0.  ...    0.    95.9    3. ]\n",
      " [   0.     0.     0.  ...    0.   436.1    3. ]\n",
      " [   0.     0.     0.  ...    0.    11.9    3. ]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
